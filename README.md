# ğŸ§  home_soft â€” Automatisation personnelle & assistant IA

Une application web simple et modulaire conÃ§ue pour :
- gÃ©rer tes **scripts personnalisÃ©s**,
- convertir des documents Markdown en PDF,
- interagir avec un **assistant IA local** (type LLM) pour lâ€™aide Ã  la programmation et lâ€™automatisation,
- le tout dans une interface web **responsive avec TailwindCSS**.

---

## ğŸš€ FonctionnalitÃ©s principales

### ğŸ”§ Gestion des scripts
- Ajouter, modifier, exÃ©cuter ou supprimer des scripts `.sh`
- Interface intuitive avec aperÃ§u au survol (description du script)
- CatÃ©gorisation dynamique via `categories.yaml`

### ğŸ§  Assistant IA (via Ollama)
- Utilise des modÃ¨les comme `codellama`, `mistral`, `phi3` en local
- Pose des questions liÃ©es Ã  la programmation, bash, Python, etc.
- Fonctionne **100% hors ligne**, aucune donnÃ©e externe nâ€™est transmise

### ğŸ“„ Conversion Markdown â†’ PDF
- TÃ©lÃ©verser ou coller ton Markdown
- Conversion directe via Pandoc + LaTeX (contenus gÃ©rÃ©s par Docker)
- Historique et logs disponibles

---

## ğŸ§° Stack technique

- **FastAPI** + **HTMX** pour un backend interactif
- **Jinja2** pour le rendu HTML
- **TailwindCSS** pour un style moderne responsive
- **Ollama** pour lâ€™IA locale
- **Pandoc** + LaTeX installÃ©s dans le conteneur

---

## âš™ï¸ Installation & Lancement

### 1. Installer Ollama

> Ollama doit tourner **sur ta machine hÃ´te**, pas dans le conteneur Docker.

```bash
curl -fsSL https://ollama.com/install.sh | sh
````

TÃ©lÃ©charger ensuite le modÃ¨le souhaitÃ© :

```bash
ollama pull codellama:7b-instruct
```

---

### 2. Lancer lâ€™application

Utilise le script de gestion inclus :

```bash
./manage.sh start
```

Ce script :

* lance lâ€™application via Docker,
* rend lâ€™interface dispo sur [http://localhost:8000](http://localhost:8000),
* et se connecte Ã  Ollama (sur lâ€™hÃ´te).

> Pour arrÃªter lâ€™application :

```bash
./manage.sh stop
```
---

### ğŸ” Changer de modÃ¨le Ollama

Par dÃ©faut, le script utilise **`codellama:7b-instruct`**.

Si tu veux utiliser un autre modÃ¨le (comme `mistral` ou `phi3`), Ã©dite le fichier `manage.sh` et modifie cette ligneâ€¯:

```bash
MODEL="codellama:7b-instruct"
```

Par exemple, pour utiliser Mistral :

```bash
MODEL="deepseek-r1:32b"
```

Assure-toi que le modÃ¨le est bien tÃ©lÃ©chargÃ©â€¯:

```bash
ollama pull deepseek-r1:32b
```

Puis relance lâ€™application :

```bash
./manage.sh restart
```
---

## ğŸ“ Structure

```
.
â”œâ”€â”€ app
â”‚Â Â  â”œâ”€â”€ categories.yaml
â”‚Â Â  â”œâ”€â”€ converter.py
â”‚Â Â  â”œâ”€â”€ main.py
â”‚Â Â  â”œâ”€â”€ requirements.txt
â”‚Â Â  â””â”€â”€ templates/
â”‚Â Â      â”œâ”€â”€ dashboard.html
â”‚Â Â      â””â”€â”€ sections/
â”‚Â Â          â”œâ”€â”€ conversion.html
â”‚Â Â          â”œâ”€â”€ default.html
â”‚Â Â          â”œâ”€â”€ edit_script.html
â”‚Â Â          â”œâ”€â”€ generation.html
â”‚Â Â          â”œâ”€â”€ history.html
â”‚Â Â          â””â”€â”€ scripts.html
â”œâ”€â”€ data/
â”‚Â Â  â”œâ”€â”€ history/
â”‚Â Â  â”‚Â Â  â””â”€â”€ conversations.json
â”‚Â Â  â”œâ”€â”€ logs/
â”‚Â Â  â”‚Â Â  â””â”€â”€ conversions.log
â”‚Â Â  â”œâ”€â”€ outputs/
â”‚Â Â  â”œâ”€â”€ pasted/
â”‚Â Â  â”œâ”€â”€ scripts/
â”‚Â Â  â”‚Â Â  â””â”€â”€ test_ollama.sh
â”‚Â Â  â””â”€â”€ uploads/
â”œâ”€â”€ manage.sh
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ Dockerfile
```

---

## âœ… TODO

* [x] Ã‰diteur complet de scripts
* [x] Assistant IA local
* [x] Menu dynamique par `categories.yaml`
* [x] Sauvegarde des conversations
* [ ] Authentification (Ã  venir)
* [ ] SystÃ¨me de thÃ¨mes
* [ ] ExÃ©cution sÃ©curisÃ©e en sandbox

---

## ğŸ›¡ï¸ SÃ©curitÃ©

âš ï¸ Ne pas exposer publiquement sans :

* authentification (FastAPI Users, etc.),
* restrictions sur lâ€™exÃ©cution de scripts,
* sandboxing des entrÃ©es.

---

## ğŸ“œ Licence

MIT â€” usage libre personnel & pro.

---

> Fait maison par [Kr034](https://github.com/Kr034) pour automatiser, apprendre et expÃ©rimenter.

````
